# OMOP Agent Pipeline

A natural language interface for querying OMOP CDM databases, built using LangChain and Streamlit.

## Database Foundation

This project is built on top of an OMOP Common Data Model (CDM) database populated with synthetic patient data generated by [Synthea](https://github.com/synthetichealth/synthea). The OMOP CDM provides a standardized way to represent healthcare data, while Synthea generates realistic synthetic patient records that maintain the statistical properties of real-world data.

### OMOP Common Data Model (CDM)
- Standardized schema for healthcare data
- Enables cross-database analysis
- Supports various data types:
  - Conditions (diagnoses)
  - Procedures
  - Observations
  - Drug exposures
  - Visit occurrences
  - Person demographics

### Synthea Synthetic Data
- Generates realistic synthetic patient records
- Maintains statistical properties of real-world data
- Includes:
  - Patient demographics
  - Medical conditions
  - Procedures
  - Medications
  - Healthcare encounters
  - Social determinants of health

This combination of OMOP CDM and Synthea data provides a robust foundation for developing and testing LLM agents that can:
- Understand healthcare queries
- Generate accurate SQL
- Handle complex medical concepts
- Process temporal relationships
- Maintain data privacy (using synthetic data)

## Database Setup

### Option 1: Use Existing Database (Current Setup)
The current implementation uses a local PostgreSQL database with OMOP CDM schema and Synthea data. To use this setup:

1. Contact the repository owner for:
   - Database backup file
   - Database credentials
   - Connection details

2. Set up your `.env` file with the provided credentials:
   ```
   DATABASE_URI=postgresql://username:password@localhost:5432/omop_cdm
   ```

### Option 2: Set Up New Database
To set up your own database:

1. Install PostgreSQL
2. Create a new database
3. Set up OMOP CDM schema:
   ```bash
   # Clone OHDSI/CommonDataModel
   git clone https://github.com/OHDSI/CommonDataModel.git
   
   # Run the PostgreSQL scripts to create the schema
   psql -d your_database_name -f CommonDataModel/PostgreSQL/OMOP CDM postgresql ddl.txt
   ```

4. Generate and load Synthea data:
   ```bash
   # Clone Synthea
   git clone https://github.com/synthetichealth/synthea.git
   
   # Generate synthetic data
   ./run_synthea -p 1000  # Generate 1000 patients
   
   # Convert to OMOP CDM format using Synthea-to-OMOP
   # (Follow instructions at https://github.com/OHDSI/ETL-Synthea)
   ```

5. Set up your `.env` file:
   ```
   DATABASE_URI=postgresql://your_username:your_password@localhost:5432/your_database_name
   ```

## Current Architecture

The agent chain consists of five main components:

1. **Input Parser Agent**
   - Parses natural language questions into structured data
   - Extracts conditions, visit types, age filters, and years
   - Uses regex patterns for reliable extraction

2. **Concept Resolver Agent**
   - Maps conditions and visit types to OMOP concept IDs
   - Handles special cases like hypertension (multiple concept IDs)
   - Supports diabetes, hypertension, ER visits, and hospitalizations

3. **SQL Generator Agent**
   - Generates SQL queries based on resolved concepts
   - Supports various query types:
     - Single conditions (e.g., diabetes only)
     - Multiple conditions (e.g., diabetes and hypertension)
     - Visit types (ER visits, hospitalizations)
     - Combined queries (conditions with visit types)

4. **Query Executor Agent**
   - Executes SQL queries using SQLAlchemy and Pandas
   - Handles database connections securely
   - Returns results as Pandas DataFrames

5. **Summarizer Agent**
   - Formats query results into readable tables
   - Provides yearly breakdowns
   - Calculates totals and percentages

## Current Technologies

- **Database Access**: Direct SQL queries to OMOP CDM tables using SQLAlchemy
- **Data Processing**: Pandas for query execution and data manipulation
- **Web Interface**: Streamlit for interactive querying
- **Natural Language Processing**: Custom regex-based parsing
- **Agent Framework**: LangChain for agent orchestration

## Future Enhancements

### 1. OMOP Integration
- Integrate [pyomop](https://github.com/OHDSI/pyomop) for:
  - Standardized OMOP concept handling
  - Built-in OMOP query templates
  - Better concept hierarchy support
  - Improved vocabulary management

### 2. Advanced Agent Framework
- Migrate to [CrewAI](https://github.com/joaomdmoura/crewAI) for:
  - More sophisticated agent collaboration
  - Better task decomposition
  - Improved error handling
  - Enhanced agent communication

### 3. Statistical Analysis Agent
- Add statistical capabilities:
  - Trend analysis over time
  - Patient demographics analysis
  - Comorbidity analysis
  - Risk factor identification
  - Statistical significance testing

### 4. Report Generation Agent
- Implement automated report generation:
  - PDF report creation
  - Data visualization
  - Executive summaries
  - Key findings extraction
  - Customizable report templates

### 5. Enhanced Natural Language Understanding
- Improve question parsing:
  - Support for more complex queries
  - Better handling of temporal relationships
  - Improved condition combinations
  - Support for medication queries
  - Procedure-based queries

### 6. Data Quality and Validation
- Add data quality checks:
  - Data completeness validation
  - Consistency checks
  - Outlier detection
  - Data quality metrics
  - Automated data cleaning

### 7. Data Profiling and ETL Tools
- Integrate [WhiteRabbit-In-A-Hat](https://github.com/OHDSI/WhiteRabbit) for:
  - Source data profiling
  - Data quality assessment
  - Field mapping assistance
  - Data type validation
  - Value distribution analysis

- Implement [Jackalope](https://github.com/OHDSI/Jackalope) for:
  - Automated ETL process generation
  - Source-to-target mapping
  - Data transformation rules
  - ETL workflow automation
  - Data quality monitoring

These tools will enhance the data pipeline by:
- Enabling mapping of new data sources into OMOP-CDM format
- Providing better data quality insights
- Automating ETL processes
- Ensuring data consistency
- Supporting data governance
- Facilitating data mapping

## Getting Started

1. Clone the repository
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Set up your `.env` file with database credentials
4. Run the Streamlit app:
   ```bash
   streamlit run app.py
   ```

## Testing

Run individual test scripts:
```bash
python test_diabetes.py
python test_hypertension.py
python test_diabetes_hypertension.py
python test_er_visits.py
python test_hospitalizations.py
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
