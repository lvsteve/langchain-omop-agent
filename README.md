# OMOP Agent Pipeline

A natural language interface for querying OMOP CDM databases, built using LangChain and Streamlit.

## Database Foundation

This project is built on top of an OMOP Common Data Model (CDM) database populated with synthetic patient data generated by [Synthea](https://github.com/synthetichealth/synthea). The OMOP CDM provides a standardized way to represent healthcare data, while Synthea generates realistic synthetic patient records that maintain the statistical properties of real-world data.

### OMOP Common Data Model (CDM)
- Standardized schema for healthcare data
- Enables cross-database analysis
- Supports various data types:
  - Conditions (diagnoses)
  - Procedures
  - Observations
  - Drug exposures
  - Visit occurrences
  - Person demographics

### Synthea Synthetic Data
- Generates realistic synthetic patient records
- Maintains statistical properties of real-world data
- Includes:
  - Patient demographics
  - Medical conditions
  - Procedures
  - Medications
  - Healthcare encounters
  - Social determinants of health

This combination of OMOP CDM and Synthea data provides a robust foundation for developing and testing LLM agents that can:
- Understand healthcare queries
- Generate accurate SQL
- Handle complex medical concepts
- Process temporal relationships
- Maintain data privacy (using synthetic data)

## Database Setup

### Prerequisites
1. Install R and required packages:
   ```R
   install.packages(c("devtools", "DatabaseConnector", "SqlRender"))
   devtools::install_github("OHDSI/ETL-Synthea")
   ```

2. Install PostgreSQL and create a new database:
   ```bash
   createdb omop_cdm
   ```

### Step 1: Set Up Synthea Schema
1. Clone Synthea:
   ```bash
   git clone https://github.com/synthetichealth/synthea.git
   cd synthea
   ```

2. Generate synthetic data:
   ```bash
   ./run_synthea -p 1000  # Generate 1000 patients
   ```

3. Create Synthea schema in PostgreSQL:
   ```R
   library(ETLSyntheaBuilder)
   createSyntheaTables(connectionDetails)
   ```

### Step 2: Set Up OMOP CDM Schema
1. Clone OHDSI Common Data Model:
   ```bash
   git clone https://github.com/OHDSI/CommonDataModel.git
   ```

2. Create OMOP CDM tables:
   ```bash
   psql -d omop_cdm -f CommonDataModel/PostgreSQL/OMOP\ CDM\ postgresql\ ddl.txt
   ```

### Step 3: Load Vocabulary
1. Download vocabulary files from [Athena](https://athena.ohdsi.org/):
   - CONCEPT.csv
   - CONCEPT_RELATIONSHIP.csv
   - CONCEPT_ANCESTOR.csv
   - CONCEPT_SYNONYM.csv
   - VOCABULARY.csv
   - CONCEPT_CLASS.csv
   - DOMAIN.csv
   - RELATIONSHIP.csv

2. Load vocabulary into database:
   ```R
   library(ETLSyntheaBuilder)
   loadVocabulary(connectionDetails, vocabPath = "path/to/vocabulary/files")
   ```

### Step 4: Load Synthea Data into OMOP CDM
1. Convert Synthea data to OMOP CDM format:
   ```R
   library(ETLSyntheaBuilder)
   etlSynthea(connectionDetails,
             syntheaSchema = "synthea",
             cdmSchema = "cdm",
             syntheaVersion = "2.7.0",
             syntheaFileLoc = "path/to/synthea/output/csv")
   ```

### Step 5: AWS Setup
1. Create an AWS RDS PostgreSQL instance:
   - Choose PostgreSQL as the engine
   - Select appropriate instance size (recommended: db.t3.large or larger)
   - Enable Multi-AZ for production environments
   - Configure security groups to allow access from your application

2. Configure security groups and network access:
   - Create a VPC if you don't have one
   - Set up security groups to allow PostgreSQL traffic (port 5432)
   - Configure network ACLs for additional security
   - Set up IAM roles for database access

3. Migrate data to AWS:
   ```bash
   pg_dump -h localhost -U username -d omop_cdm | psql -h your-aws-endpoint -U username -d omop_cdm
   ```

4. Set up your `.env` file with AWS credentials:
   ```
   DATABASE_URI=postgresql://username:password@your-aws-endpoint:5432/omop_cdm
   ```

5. Deploy the application to AWS:
   - Set up an EC2 instance or use AWS Elastic Beanstalk
   - Configure environment variables
   - Set up proper security groups for the application
   - Configure auto-scaling if needed

### Step 6: Verify Setup
1. Run test queries to verify data:
   ```sql
   SELECT COUNT(*) FROM cdm.person;
   SELECT COUNT(*) FROM cdm.condition_occurrence;
   SELECT COUNT(*) FROM cdm.visit_occurrence;
   ```

2. Run the test scripts:
   ```bash
   python test_diabetes.py
   python test_hypertension.py
   python test_diabetes_hypertension.py
   python test_er_visits.py
   python test_hospitalizations.py
   ```

## Current Architecture

The agent chain consists of five main components:

1. **Input Parser Agent**
   - Parses natural language questions into structured data
   - Extracts conditions, visit types, age filters, and years
   - Uses regex patterns for reliable extraction

2. **Concept Resolver Agent**
   - Maps conditions and visit types to OMOP concept IDs
   - Handles special cases like hypertension (multiple concept IDs)
   - Supports diabetes, hypertension, ER visits, and hospitalizations

3. **SQL Generator Agent**
   - Generates SQL queries based on resolved concepts
   - Supports various query types:
     - Single conditions (e.g., diabetes only)
     - Multiple conditions (e.g., diabetes and hypertension)
     - Visit types (ER visits, hospitalizations)
     - Combined queries (conditions with visit types)

4. **Query Executor Agent**
   - Executes SQL queries using SQLAlchemy and Pandas
   - Handles database connections securely
   - Returns results as Pandas DataFrames

5. **Summarizer Agent**
   - Formats query results into readable tables
   - Provides yearly breakdowns
   - Calculates totals and percentages

## Current Technologies

- **Database Access**: Direct SQL queries to OMOP CDM tables using SQLAlchemy
- **Data Processing**: Pandas for query execution and data manipulation
- **Web Interface**: 
  - Main Streamlit app for natural language querying
  - Secondary Streamlit dashboard for event rate analysis (ER visits and hospitalizations)
- **Natural Language Processing**: Custom regex-based parsing
- **Agent Framework**: LangChain for agent orchestration

## Future Enhancements

### 1. OMOP Integration
- Integrate [pyomop](https://github.com/OHDSI/pyomop) for:
  - Standardized OMOP concept handling
  - Built-in OMOP query templates
  - Better concept hierarchy support
  - Improved vocabulary management

### 2. Advanced Agent Framework
- Migrate to [CrewAI](https://github.com/joaomdmoura/crewAI) for:
  - More sophisticated agent collaboration
  - Better task decomposition
  - Improved error handling
  - Enhanced agent communication

### 3. Statistical Analysis Agent
- Add statistical capabilities:
  - Trend analysis over time
  - Patient demographics analysis
  - Comorbidity analysis
  - Risk factor identification
  - Statistical significance testing

### 4. Report Generation Agent
- Implement automated report generation:
  - PDF report creation
  - Data visualization
  - Executive summaries
  - Key findings extraction
  - Customizable report templates

### 5. Enhanced Natural Language Understanding
- Improve question parsing:
  - Support for more complex queries
  - Better handling of temporal relationships
  - Improved condition combinations
  - Support for medication queries
  - Procedure-based queries

### 6. Data Quality and Validation
- Add data quality checks:
  - Data completeness validation
  - Consistency checks
  - Outlier detection
  - Data quality metrics
  - Automated data cleaning

### 7. Data Profiling and ETL Tools
- Integrate [WhiteRabbit-In-A-Hat](https://github.com/OHDSI/WhiteRabbit) for:
  - Source data profiling
  - Data quality assessment
  - Field mapping assistance
  - Data type validation
  - Value distribution analysis

- Implement [Jackalope](https://github.com/OHDSI/Jackalope) for:
  - Automated ETL process generation
  - Source-to-target mapping
  - Data transformation rules
  - ETL workflow automation
  - Data quality monitoring

These tools will enhance the data pipeline by:
- Enabling mapping of new data sources into OMOP-CDM format
- Providing better data quality insights
- Automating ETL processes
- Ensuring data consistency
- Supporting data governance
- Facilitating data mapping

## Getting Started

1. Clone the repository
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Run the main Streamlit app for natural language querying:
   ```bash
   streamlit run app.py --server.port 8501 --server.address 127.0.0.1
   ```

4. Run the rates dashboard app in a separate terminal:
   ```bash
   streamlit run rates/streamlit_app.py --server.port 8502 --server.address 127.0.0.1
   ```

5. Access the apps in your browser:
   - Main app: http://127.0.0.1:8501
   - Rates dashboard: http://127.0.0.1:8502

## Testing

Run individual test scripts:
```bash
python test_diabetes.py
python test_hypertension.py
python test_diabetes_hypertension.py
python test_er_visits.py
python test_hospitalizations.py
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
